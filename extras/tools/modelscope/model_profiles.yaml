profiles:
  reasoning-thinker:
    description: DeepSeek-R1 reasoning model served from ModelScope assets.
    env:
      MODELSCOPE_CACHE: /home/vllmuser/.cache/modelscope/reasoning
      VLLM_USE_MODELSCOPE: "1"
      PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
    serve:
      entrypoint: python -m vllm.entrypoints.openai.api_server
      args:
        - --distributed-executor-backend mp
        - --model deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
        - --served-model-name deepseek-r1-8b
        - --max-model-len 4096
        - --dtype bfloat16
        - --generation-config vllm
        - --kv-cache-dtype fp8
        - --quantization-param-path
        - /home/vllmuser/kvdata/deepseek-r1/kv_cache_scales.json
        - --max-num-seqs 1
        - --max-num-batched-tokens 768
        - --reasoning-parser deepseek_r1
        - --trust-remote-code
        - --gpu-memory-utilization 0.50
        - --swap-space 12
    kv_calibration:
      model_id: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
      output: /home/vllmuser/kvdata/deepseek-r1/kv_cache_scales.json
      calib_data: /home/vllmuser/kvdata/calib/snippets.jsonl
      samples: 128
      seq_len: 4096
      dataset_prompt: Calibration sample {i}. Short text for KV scales.
      cache_dir: /home/vllmuser/.cache/modelscope
      llm_compressor_repo: /opt/llm-compressor
      quantization_script: >-
        examples/quantization_non_uniform/quantization_multiple_modifiers.py
      quant_args: []

  vl-thinker:
    description: Qwen3 VL thinking model with precomputed KV scales.
    env:
      MODELSCOPE_CACHE: /home/vllmuser/.cache/modelscope/vl
      VLLM_USE_MODELSCOPE: "1"
      PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
    serve:
      entrypoint: python -m vllm.entrypoints.openai.api_server
      args:
        - --distributed-executor-backend mp
        - --model Qwen/Qwen3-VL-8B-Thinking-FP8
        - --served-model-name qwen3-vl-8b-thinking
        - --max-model-len 4096
        - --dtype bfloat16
        - --generation-config vllm
        - --kv-cache-dtype fp8
        - --quantization-param-path
        - /home/vllmuser/kvdata/qwen3-vl/kv_cache_scales.json
        - --max-num-seqs 1
        - --max-num-batched-tokens 768
        - --reasoning-parser qwen3
        - --enforce-eager
        - --trust-remote-code
        - --gpu-memory-utilization 0.50
        - --limit_mm_per_prompt {"image":3,"video":1}
        - --swap-space 12
    kv_calibration:
      model_id: Qwen/Qwen3-VL-8B-Thinking-FP8
      output: /home/vllmuser/kvdata/qwen3-vl/kv_cache_scales.json
      calib_data: /home/vllmuser/kvdata/calib/snippets.jsonl
      samples: 128
      seq_len: 4096
      dataset_prompt: Calibration sample {i}. Short text for KV scales.
      cache_dir: /home/vllmuser/.cache/modelscope
      llm_compressor_repo: /opt/llm-compressor
      quantization_script: >-
        examples/quantization_non_uniform/quantization_multiple_modifiers.py
      quant_args: []

  embedding-service:
    description: Qwen3 embedding endpoint served via OpenAI-compatible API.
    env:
      MODELSCOPE_CACHE: /home/vllmuser/.cache/modelscope
      VLLM_USE_MODELSCOPE: "1"
      PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
    serve:
      entrypoint: python -m vllm.entrypoints.openai.api_server
      args:
        - --distributed-executor-backend mp
        - --model Qwen/Qwen3-Embedding-8B
        - --served-model-name qwen3-embedding-8b
        - --max-model-len 4096
        - --dtype bfloat16
        - --generation-config vllm
        - --kv-cache-dtype fp8
        - --max-num-seqs 4
        - --max-num-batched-tokens 768
        - --trust-remote-code
