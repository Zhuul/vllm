# vLLM Development Container (UBI9 + CUDA)

This directory contains tools for setting up a vLLM development environment using Podman containers with NVIDIA CUDA on Red Hat UBI9 base.

## Features

- **UBI9 + CUDA 12.9.1**: Latest CUDA with cuDNN on Red Hat Universal Base Image (Fedora ecosystem)
- **Python Virtual Environment**: Modern, isolated Python environment following best practices
- **GPU support**: Full CUDA development toolkit for GPU acceleration
- **Editable install**: Changes to Python code are immediately reflected
- **Persistent caches**: Hugging Face models and vLLM cache persist between container runs
- **Non-root user**: Secure development environment with proper virtual environment
- **SSH access**: Remote development support
- **Flexible networking**: Use existing networks or create new ones

## Prerequisites

- **Podman**: Install Podman Desktop or Podman CLI
- **GPU support** (optional): NVIDIA Container Toolkit configured
- **Your vLLM fork**: Clone of https://github.com/Zhuul/vllm

## Network Configuration

The scripts use **`llm-net`** as the default Podman network, which can be customized:

### Environment Variable
Set `VLLM_PODMAN_NETWORK` to use a different network:

**Windows:**
```powershell
$env:VLLM_PODMAN_NETWORK = "my-custom-network"
.\extras\run-vllm-dev-fedora.ps1
```

**Linux:**
```bash
export VLLM_PODMAN_NETWORK="my-custom-network"
./extras/run-vllm-dev-fedora.sh
```

### Network Behavior
- **Network exists**: Scripts will use the existing network
- **Network doesn't exist**: Scripts will create it automatically
- **Creation fails**: Scripts fall back to default Podman networking

## Quick Start

### Windows (PowerShell)

1. **Configure paths** in `run-vllm-dev-fedora.ps1`:
   ```powershell
   $VLLMSourcePath   = 'C:\path\to\your\vllm\fork'
   $ModelCacheVolume = 'C:\models\huggingface'
   ```

2. **Set your Hugging Face token**:
   ```powershell
   $EnvToken = 'HUGGINGFACE_HUB_TOKEN=your_actual_token_here'
   ```

3. **Optional - Set custom network**:
   ```powershell
   $env:VLLM_PODMAN_NETWORK = "llm-net"  # or your preferred network
   ```

4. **Run from vLLM repository root**:
   ```powershell
   .\extras\run-vllm-dev-fedora.ps1
   ```

### Linux (Bash)

1. **Configure paths** in `run-vllm-dev-fedora.sh`:
   ```bash
   VLLM_SOURCE_PATH="${HOME}/projects/vllm"
   MODEL_CACHE_VOLUME="${HOME}/.cache/huggingface"
   ```

2. **Set your Hugging Face token**:
   ```bash
   export HUGGINGFACE_HUB_TOKEN="your_actual_token_here"
   ```

3. **Optional - Set custom network**:
   ```bash
   export VLLM_PODMAN_NETWORK="llm-net"  # or your preferred network
   ```

4. **Make executable and run**:
   ```bash
   chmod +x extras/run-vllm-dev-fedora.sh
   ./extras/run-vllm-dev-fedora.sh
   ```

## What the Scripts Do

1. **Check/create network** - Verifies if the specified network exists, creates if needed
2. **Build container image** from Dockerfile with:
   - NVIDIA CUDA 12.9.1 + cuDNN on UBI9 base
   - Python 3 with isolated virtual environment at `/home/vllmuser/venv`
   - PyTorch with CUDA support pre-installed
   - Development tools and dependencies
3. **Create development container** with:
   - Your vLLM source mounted at `/workspace`
   - Persistent Hugging Face cache
   - Persistent vLLM cache
   - SSH server (port 2222)
   - API server access (port 8000)
   - Connection to specified network
   - Virtual environment automatically activated
4. **Install vLLM** in editable mode (`pip install -e .`) in the virtual environment
5. **Test installation** with a simple import check

## Virtual Environment

The container uses a modern Python virtual environment setup:

- **Location**: `/home/vllmuser/venv`
- **Auto-activation**: Virtual environment is automatically activated in interactive sessions
- **Isolation**: All Python packages are installed in the virtual environment, not system-wide
- **Best practices**: No root pip warnings, clean dependency management

### Virtual Environment Commands

```bash
# Check virtual environment status
./extras/check-venv.sh

# Manual activation (if needed)
source /home/vllmuser/venv/bin/activate

# Verify activation
echo $VIRTUAL_ENV  # Should show: /home/vllmuser/venv
```

## Development Workflow

### Making Changes

1. **Edit code** on your host using your preferred editor
2. **Test changes** in the container - Python changes are immediate
3. **Rebuild extensions** if you change C++/CUDA code:
   ```bash
   cd /workspace
   pip install -e .
   ```

### Testing vLLM

```bash
# Quick test
python3 -c "import vllm; print(vllm.__version__)"

# Start API server
vllm serve facebook/opt-125m --host 0.0.0.0 --port 8000

# Test API (from host)
curl -X POST "http://localhost:8000/v1/completions" \
     -H "Content-Type: application/json" \
     -d '{"model": "facebook/opt-125m", "prompt": "Hello!", "max_tokens": 5}'
```

### Container Management

```bash
# Reconnect to running container
podman start -ai vllm-dev-fedora

# Stop container
podman stop vllm-dev-fedora

# Remove container (keeps image)
podman rm vllm-dev-fedora

# Remove image (for clean rebuild)
podman rmi vllm-dev-fedora:latest

# Check network information
./extras/manage-container.sh network
```

## Configuration Options

### Environment Variables

**Network Configuration:**
- `VLLM_PODMAN_NETWORK`: Override default network (default: `llm-net`)

**Runtime Configuration:**
- `VLLM_USE_V1=1`: Enable vLLM V1 features
- `VLLM_DISABLE_FLASH_ATTN=1`: Disable flash attention if build issues
- `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`: Optimize GPU memory

### Build Options

- **CPU-only build**: Remove CUDA base image and use `fedora:42`
- **Different PyTorch version**: Modify versions in Dockerfile
- **Additional packages**: Add to Dockerfile RUN commands

## Networking Examples

### Using Existing Network
If you already have a `llm-net` network for other containers:
```bash
# Linux
export VLLM_PODMAN_NETWORK="llm-net"
./extras/run-vllm-dev-fedora.sh
```

### Creating Project-Specific Network
```bash
# Create network manually
podman network create my-vllm-net

# Use it with the script
export VLLM_PODMAN_NETWORK="my-vllm-net"
./extras/run-vllm-dev-fedora.sh
```

### Default Networking
```bash
# Use default Podman networking (no custom network)
export VLLM_PODMAN_NETWORK=""
./extras/run-vllm-dev-fedora.sh
```

## Troubleshooting

### Common Issues

1. **Build fails**: Check if base image exists and network connection
2. **Permission errors**: Ensure `:Z` suffix on volume mounts for SELinux
3. **GPU not detected**: Verify NVIDIA Container Toolkit installation
4. **SSH connection fails**: Check if port 2222 is available
5. **Network issues**: Check if network exists with `podman network ls`

### Network Troubleshooting
```bash
# List all networks
podman network ls

# Inspect specific network
podman network inspect llm-net

# Check container network
podman inspect vllm-dev-fedora | grep -A 10 NetworkSettings
```

### Getting Help

- Check container logs: `podman logs vllm-dev-fedora`
- Connect to container: `podman exec -it vllm-dev-fedora /bin/bash`
- Check network info: `./extras/manage-container.sh network`
- Check vLLM documentation: [docs.vllm.ai](https://docs.vllm.ai)

## Customization

You can modify the Dockerfile and scripts for your specific needs:

- Add development tools to the Dockerfile
- Mount additional directories
- Change port mappings
- Add environment variables
- Customize the container setup commands
- Use different networks for different projects

The scripts are designed to be easily modified for different development setups while maintaining compatibility with existing network configurations.