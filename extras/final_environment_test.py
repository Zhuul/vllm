#!/usr/bin/env python3
"""
vLLM Development Environment - Final Verification Test
This script verifies that the complete vLLM development environment is working correctly.
"""

import sys
import os

def main():
    print("=" * 60)
    print("üöÄ vLLM Development Environment - Final Test")
    print("=" * 60)
    print(f"Python: {sys.version}")
    print(f"Working directory: {os.getcwd()}")
    
    # Test 1: GPU and PyTorch
    print("\n1Ô∏è‚É£ Testing GPU and PyTorch...")
    try:
        import torch
        print(f"   ‚úÖ PyTorch: {torch.__version__}")
        print(f"   ‚úÖ CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}")
            print(f"   ‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory // (1024**3)}GB")
            gpu_ok = True
        else:
            print("   ‚ùå No GPU detected")
            gpu_ok = False
    except Exception as e:
        print(f"   ‚ùå PyTorch/CUDA error: {e}")
        gpu_ok = False

    # Test 2: vLLM Import
    print("\n2Ô∏è‚É£ Testing vLLM Installation...")
    try:
        import vllm
        print(f"   ‚úÖ vLLM imported: {vllm.__version__}")
        print(f"   ‚úÖ Location: {vllm.__file__}")
        vllm_ok = True
    except Exception as e:
        print(f"   ‚ùå vLLM import failed: {e}")
        vllm_ok = False

    # Test 3: vLLM Core Classes
    if vllm_ok:
        print("\n3Ô∏è‚É£ Testing vLLM Core Classes...")
        try:
            from vllm import LLM, SamplingParams
            print("   ‚úÖ LLM class imported")
            print("   ‚úÖ SamplingParams class imported")
            classes_ok = True
        except Exception as e:
            print(f"   ‚ùå vLLM classes failed: {e}")
            classes_ok = False
    else:
        classes_ok = False

    # Final Results
    print("\n" + "="*60)
    print("üìä FINAL RESULTS:")
    print(f"   GPU/PyTorch: {'‚úÖ PASS' if gpu_ok else '‚ùå FAIL'}")
    print(f"   vLLM Import: {'‚úÖ PASS' if vllm_ok else '‚ùå FAIL'}")
    print(f"   vLLM Classes: {'‚úÖ PASS' if classes_ok else '‚ùå FAIL'}")
    
    all_ok = gpu_ok and vllm_ok and classes_ok
    
    if all_ok:
        print("\nüéâ SUCCESS: vLLM development environment is ready!")
        print("\nüìã Next Steps:")
        print("   ‚Ä¢ Load a model: llm = vllm.LLM('facebook/opt-125m')")
        print("   ‚Ä¢ Generate text: outputs = llm.generate(['Hello!'])")
        print("   ‚Ä¢ Start API server: python -m vllm.entrypoints.openai.api_server")
        return 0
    else:
        print("\n‚ùå FAILED: Environment has issues that need to be resolved")
        return 1

if __name__ == "__main__":
    sys.exit(main())
